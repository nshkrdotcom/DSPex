# Generated by SnakeBridge v0.14.0 - DO NOT EDIT MANUALLY
# Regenerate with: mix compile
# Library: dspy 3.1.2
# Python module: dspy.teleprompt.gepa.gepa_utils
# Python class: DspyAdapter

defmodule Dspy.Teleprompt.Gepa.GepaUtils.DspyAdapter do
  @moduledoc """
  GEPAAdapter is the single integration point between your system

  and the GEPA optimization engine. Implementers provide three responsibilities:

  The following are user-defined types that are not interpreted by GEPA but are used by the user's code
      to define the adapter:
  DataInst: User-defined type of input data to the program under optimization.
  Trajectory: User-defined type of trajectory data, which typically captures the
      different steps of the program candidate execution.
  RolloutOutput: User-defined type of output data from the program candidate.

  The following are the responsibilities of the adapter:
  1) Program construction and evaluation (evaluate):
     Given a batch of DataInst and a "candidate" program (mapping from named components
     -> component text), execute the program to produce per-example scores and
     optionally rich trajectories (capturing intermediate states) needed for reflection.

  2) Reflective dataset construction (make_reflective_dataset):
     Given the candidate, EvaluationBatch (trajectories, outputs, scores), and the list of components to update,
     produce a small JSON-serializable dataset for each component that you want to update. This
     dataset is fed to the teacher LM to propose improved component text.

  3) Optional instruction proposal (propose_new_texts):
     GEPA provides a default implementation (instruction_proposal.py) that serializes the reflective dataset
     to propose new component texts. However, users can implement their own proposal logic by implementing this method.
     This method receives the current candidate, the reflective dataset, and the list of components to update,
     and returns a mapping from component name to new component text.

  Key concepts and contracts:
  - candidate: Dict[str, str] mapping a named component of the system to its corresponding text.
  - scores: higher is better. GEPA uses:
    - minibatch: sum(scores) to compare old vs. new candidate (acceptance test),
    - full valset: mean(scores) for tracking and Pareto-front selection.
    Ensure your metric is calibrated accordingly or normalized to a consistent scale.
  - trajectories: opaque to GEPA (the engine never inspects them). They must be
    consumable by your own make_reflective_dataset implementation to extract the
    minimal context needed to produce meaningful feedback for every component of
    the system under optimization.
  - error handling: Never raise for individual example failures. Instead:
    - Return a valid `EvaluationBatch` with per-example failure scores (e.g., 0.0)
      when formatting/parsing fails. Even better if the trajectories are also populated
      with the failed example, including the error message, identifying the reason for the failure.
    - Reserve exceptions for unrecoverable, systemic failures (e.g., missing model,
      misconfigured program, schema mismatch).
    - If an exception is raised, the engine will log the error and proceed to the next iteration.
  """
  def __snakebridge_python_name__, do: "dspy.teleprompt.gepa.gepa_utils"
  def __snakebridge_python_class__, do: "DspyAdapter"
  def __snakebridge_library__, do: "dspy"
  @opaque t :: SnakeBridge.Ref.t()

  @doc """
  Initialize self.  See help(type(self)) for accurate signature.

  ## Parameters

  - `student_module` (term())
  - `metric_fn` (term())
  - `feedback_map` (%{optional(String.t()) => term()})
  - `failure_score` (term() default: 0.0)
  - `num_threads` (term() default: None)
  - `add_format_failure_as_feedback` (boolean() default: False)
  - `rng` (term() default: None)
  - `reflection_lm` (term() default: None)
  - `custom_instruction_proposer` (term() default: None)
  - `warn_on_score_mismatch` (boolean() default: True)
  - `enable_tool_optimization` (boolean() default: False)
  - `reflection_minibatch_size` (term() default: None)
  """
  @spec new(term(), term(), %{optional(String.t()) => term()}, list(term()), keyword()) ::
          {:ok, SnakeBridge.Ref.t()} | {:error, Snakepit.Error.t()}
  def new(student_module, metric_fn, feedback_map, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_class(
      __MODULE__,
      :__init__,
      [student_module, metric_fn, feedback_map] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  Recursively collect all Tool instances from a module and its sub-modules.

  ## Parameters

  - `module` (Dspy.Primitives.ModuleClass3.t())

  ## Returns

  - `%{optional(String.t()) => Dspy.Adapters.Types.ToolClass3.t()}`
  """
  @spec _collect_tools(SnakeBridge.Ref.t(), Dspy.Primitives.ModuleClass3.t(), keyword()) ::
          {:ok, %{optional(String.t()) => Dspy.Adapters.Types.ToolClass3.t()}}
          | {:error, Snakepit.Error.t()}
  def _collect_tools(ref, module, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :_collect_tools, [module], opts)
  end

  @doc """
  Python method `DspyAdapter._update_tool_descriptions`.

  ## Parameters

  - `program` (Dspy.Primitives.ModuleClass3.t())
  - `tool_candidates` (%{optional(String.t()) => term()})

  ## Returns

  - `nil`
  """
  @spec _update_tool_descriptions(
          SnakeBridge.Ref.t(),
          Dspy.Primitives.ModuleClass3.t(),
          %{optional(String.t()) => term()},
          keyword()
        ) :: {:ok, nil} | {:error, Snakepit.Error.t()}
  def _update_tool_descriptions(ref, program, tool_candidates, opts \\ []) do
    SnakeBridge.Runtime.call_method(
      ref,
      :_update_tool_descriptions,
      [program, tool_candidates],
      opts
    )
  end

  @doc """
  Python method `DspyAdapter.build_program`.

  ## Parameters

  - `candidate` (%{optional(String.t()) => String.t()})

  ## Returns

  - `term()`
  """
  @spec build_program(SnakeBridge.Ref.t(), %{optional(String.t()) => String.t()}, keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def build_program(ref, candidate, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :build_program, [candidate], opts)
  end

  @doc """
  Run the program defined by `candidate` on a batch of data.



  ## Parameters

  - `- candidate: mapping from component name -> component text. You must instantiate your full system with the component text for each component, and execute it on the batch.` - 
  - `- capture_traces: when True, you must populate `EvaluationBatch.trajectories` with a per-example trajectory object that your `make_reflective_dataset` can later consume. When False, you may set trajectories=None to save time/memory. capture_traces=True is used by the reflective mutation proposer to build a reflective dataset.` - 
  - `Returns` - 
  - `- EvaluationBatch with: - outputs: raw per-example outputs (opaque to GEPA). - scores: per-example floats, length == len(batch). Higher is better. - trajectories: - if capture_traces=True: list[Trajectory] with length == len(batch). - if capture_traces=False: None.` - 
  - `Scoring semantics` - 
  - `- The engine uses sum(scores) on minibatches to decide whether to accept a candidate mutation and average(scores) over the full valset for tracking.` - 
  - `- Prefer to return per-example scores, that can be aggregated via summation.` - 
  - `- If an example fails (e.g., parse error), use a fallback score (e.g., 0.0).` - 
  - `Correctness constraints` - 
  - `- len(outputs) == len(scores) == len(batch)` - 
  - `- If capture_traces=True: trajectories must be provided and len(trajectories) == len(batch)` - 
  - `- Do not mutate `batch` or `candidate` in-place. Construct a fresh program instance or deep-copy as needed.` - 

  ## Returns

  Returns `Scoring semantics`.
  """
  @spec evaluate(SnakeBridge.Ref.t(), term(), term(), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def evaluate(ref, batch, candidate, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :evaluate, [batch, candidate] ++ List.wrap(args), opts)
  end

  @doc """
  Build a small, JSON-serializable dataset (per component) to drive instruction

  refinement by a teacher LLM.

  ## Parameters

  - `- eval_batch: The result of evaluate(..., capture_traces=True) on the same batch. You should extract everything you need from eval_batch.trajectories (and optionally outputs/scores) to assemble concise, high-signal examples.` - 
  - `- components_to_update: subset of component names for which the proposer has requested updates. At a time, GEPA identifies a subset of components to update.` - 
  - `Returns` - 
  - `- A dict: component_name -> list of dict records (the "reflective dataset"). Each record should be JSON-serializable and is passed verbatim to the instruction proposal prompt. A recommended schema is: { "Inputs": Dict[str, str],             # Minimal, clean view of the inputs to the component "Generated Outputs": Dict[str, str] | str,  # Model outputs or raw text "Feedback": str                       # Feedback on the component's performance, including correct answer, error messages, etc. } You may include additional keys (e.g., "score", "rationale", "trace_id") if useful.` - 
  - `Determinism` - 
  - `- If you subsample trace instances, use a seeded RNG to keep runs reproducible.` - 

  ## Returns

  Returns `Determinism`.
  """
  @spec make_reflective_dataset(SnakeBridge.Ref.t(), term(), term(), term(), keyword()) ::
          {:ok,
           %{optional(String.t()) => list(Dspy.Teleprompt.Gepa.GepaUtils.ReflectiveExample.t())}}
          | {:error, Snakepit.Error.t()}
  def make_reflective_dataset(ref, candidate, eval_batch, components_to_update, opts \\ []) do
    SnakeBridge.Runtime.call_method(
      ref,
      :make_reflective_dataset,
      [candidate, eval_batch, components_to_update],
      opts
    )
  end

  @doc """
  Python method `DspyAdapter.propose_new_texts`.

  ## Parameters

  - `candidate` (%{optional(String.t()) => String.t()})
  - `reflective_dataset` (%{optional(String.t()) => list(%{optional(String.t()) => term()})})
  - `components_to_update` (list(String.t()))

  ## Returns

  - `%{optional(String.t()) => String.t()}`
  """
  @spec propose_new_texts(
          SnakeBridge.Ref.t(),
          %{optional(String.t()) => String.t()},
          %{optional(String.t()) => list(%{optional(String.t()) => term()})},
          list(String.t()),
          keyword()
        ) :: {:ok, %{optional(String.t()) => String.t()}} | {:error, Snakepit.Error.t()}
  def propose_new_texts(ref, candidate, reflective_dataset, components_to_update, opts \\ []) do
    SnakeBridge.Runtime.call_method(
      ref,
      :propose_new_texts,
      [candidate, reflective_dataset, components_to_update],
      opts
    )
  end

  @doc """
  Python method `DspyAdapter.stripped_lm_call`.

  ## Parameters

  - `x` (String.t())

  ## Returns

  - `list(String.t())`
  """
  @spec stripped_lm_call(SnakeBridge.Ref.t(), String.t(), keyword()) ::
          {:ok, list(String.t())} | {:error, Snakepit.Error.t()}
  def stripped_lm_call(ref, x, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :stripped_lm_call, [x], opts)
  end

  @spec _abc_impl(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def _abc_impl(ref) do
    SnakeBridge.Runtime.get_attr(ref, :_abc_impl)
  end

  @spec _is_protocol(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def _is_protocol(ref) do
    SnakeBridge.Runtime.get_attr(ref, :_is_protocol)
  end

  @spec _is_runtime_protocol(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def _is_runtime_protocol(ref) do
    SnakeBridge.Runtime.get_attr(ref, :_is_runtime_protocol)
  end
end
