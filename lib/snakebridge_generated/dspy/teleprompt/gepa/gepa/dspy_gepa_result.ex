# Generated by SnakeBridge v0.14.0 - DO NOT EDIT MANUALLY
# Regenerate with: mix compile
# Library: dspy 3.1.2
# Python module: dspy.teleprompt.gepa.gepa
# Python class: DspyGEPAResult

defmodule Dspy.Teleprompt.Gepa.Gepa.DspyGEPAResult do
  @moduledoc """
  Experimental: This class may change or be removed in a future release without warning (introduced in v3.0.0).

  Additional data related to the GEPA run.

  Fields:
  - candidates: list of proposed candidates (component_name -> component_text)
  - parents: lineage info; for each candidate i, parents[i] is a list of parent indices or None
  - val_aggregate_scores: per-candidate aggregate score on the validation set (higher is better)
  - val_subscores: per-candidate per-instance scores on the validation set (len == num_val_instances)
  - per_val_instance_best_candidates: for each val instance t, a set of candidate indices achieving the best score on t
  - discovery_eval_counts: Budget (number of metric calls / rollouts) consumed up to the discovery of each candidate

  - total_metric_calls: total number of metric calls made across the run
  - num_full_val_evals: number of full validation evaluations performed
  - log_dir: where artifacts were written (if any)
  - seed: RNG seed for reproducibility (if known)

  - best_idx: candidate index with the highest val_aggregate_scores
  - best_candidate: the program text mapping for best_idx
  """
  def __snakebridge_python_name__, do: "dspy.teleprompt.gepa.gepa"
  def __snakebridge_python_class__, do: "DspyGEPAResult"
  def __snakebridge_library__, do: "dspy"
  @opaque t :: SnakeBridge.Ref.t()

  @doc """
  Initialize self.  See help(type(self)) for accurate signature.

  ## Parameters

  - `candidates` (list(Dspy.Primitives.ModuleClass3.t()))
  - `parents` (list(list(term())))
  - `val_aggregate_scores` (list(float()))
  - `val_subscores` (list(list(float())))
  - `per_val_instance_best_candidates` (list(MapSet.t(integer())))
  - `discovery_eval_counts` (list(integer()))
  - `best_outputs_valset` (term() default: None)
  - `total_metric_calls` (term() default: None)
  - `num_full_val_evals` (term() default: None)
  - `log_dir` (term() default: None)
  - `seed` (term() default: None)
  """
  @spec new(
          list(Dspy.Primitives.ModuleClass3.t()),
          list(list(term())),
          list(float()),
          list(list(float())),
          list(MapSet.t(integer())),
          list(integer()),
          list(term()),
          keyword()
        ) :: {:ok, SnakeBridge.Ref.t()} | {:error, Snakepit.Error.t()}
  def new(
        candidates,
        parents,
        val_aggregate_scores,
        val_subscores,
        per_val_instance_best_candidates,
        discovery_eval_counts,
        args,
        opts \\ []
      ) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_class(
      __MODULE__,
      :__init__,
      [
        candidates,
        parents,
        val_aggregate_scores,
        val_subscores,
        per_val_instance_best_candidates,
        discovery_eval_counts
      ] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  Python method `DspyGEPAResult.from_gepa_result`.

  ## Parameters

  - `gepa_result` (term())
  - `adapter` (Dspy.Teleprompt.Gepa.GepaUtils.DspyAdapter.t())

  ## Returns

  - `Dspy.Teleprompt.Gepa.Gepa.DspyGEPAResult.t()`
  """
  @spec from_gepa_result(
          SnakeBridge.Ref.t(),
          term(),
          Dspy.Teleprompt.Gepa.GepaUtils.DspyAdapter.t(),
          keyword()
        ) :: {:ok, Dspy.Teleprompt.Gepa.Gepa.DspyGEPAResult.t()} | {:error, Snakepit.Error.t()}
  def from_gepa_result(ref, gepa_result, adapter, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :from_gepa_result, [gepa_result, adapter], opts)
  end

  @doc """
  Python method `DspyGEPAResult.to_dict`.

  ## Returns

  - `%{optional(String.t()) => term()}`
  """
  @spec to_dict(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, %{optional(String.t()) => term()}} | {:error, Snakepit.Error.t()}
  def to_dict(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :to_dict, [], opts)
  end

  @spec best_candidate(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def best_candidate(ref) do
    SnakeBridge.Runtime.get_attr(ref, :best_candidate)
  end

  @spec best_idx(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def best_idx(ref) do
    SnakeBridge.Runtime.get_attr(ref, :best_idx)
  end

  @spec best_outputs_valset(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def best_outputs_valset(ref) do
    SnakeBridge.Runtime.get_attr(ref, :best_outputs_valset)
  end

  @spec highest_score_achieved_per_val_task(SnakeBridge.Ref.t()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def highest_score_achieved_per_val_task(ref) do
    SnakeBridge.Runtime.get_attr(ref, :highest_score_achieved_per_val_task)
  end

  @spec log_dir(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def log_dir(ref) do
    SnakeBridge.Runtime.get_attr(ref, :log_dir)
  end

  @spec num_full_val_evals(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def num_full_val_evals(ref) do
    SnakeBridge.Runtime.get_attr(ref, :num_full_val_evals)
  end

  @spec seed(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def seed(ref) do
    SnakeBridge.Runtime.get_attr(ref, :seed)
  end

  @spec total_metric_calls(SnakeBridge.Ref.t()) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def total_metric_calls(ref) do
    SnakeBridge.Runtime.get_attr(ref, :total_metric_calls)
  end
end
