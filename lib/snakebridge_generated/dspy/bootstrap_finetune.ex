# Generated by SnakeBridge v0.15.0 - DO NOT EDIT MANUALLY
# Regenerate with: mix compile
# Library: dspy 3.1.2
# Python module: dspy
# Python class: BootstrapFinetune

defmodule Dspy.BootstrapFinetune do
  @moduledoc """
  Wrapper for Python class BootstrapFinetune.
  """
  def __snakebridge_python_name__, do: "dspy"
  def __snakebridge_python_class__, do: "BootstrapFinetune"
  def __snakebridge_library__, do: "dspy"
  @opaque t :: SnakeBridge.Ref.t()

  @doc """
  Initialize self.  See help(type(self)) for accurate signature.

  ## Parameters

  - `metric` (term() | nil default: None)
  - `multitask` (boolean() default: True)
  - `train_kwargs` (term() default: None)
  - `adapter` (term() default: None)
  - `exclude_demos` (boolean() default: False)
  - `num_threads` (term() default: None)
  """
  @spec new(list(term()), keyword()) :: {:ok, SnakeBridge.Ref.t()} | {:error, Snakepit.Error.t()}
  def new(args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_class(__MODULE__, :__init__, [] ++ List.wrap(args), opts)
  end

  @doc """
  Python method `BootstrapFinetune._prepare_finetune_data`.

  ## Parameters

  - `trace_data` (list(%{optional(String.t()) => term()}))
  - `lm` (term())
  - `pred_ind` (term() default: None)

  ## Returns

  - `term()`
  """
  @spec _prepare_finetune_data(
          SnakeBridge.Ref.t(),
          list(%{optional(String.t()) => term()}),
          term(),
          list(term()),
          keyword()
        ) :: {:ok, term()} | {:error, Snakepit.Error.t()}
  def _prepare_finetune_data(ref, trace_data, lm, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)

    SnakeBridge.Runtime.call_method(
      ref,
      :_prepare_finetune_data,
      [trace_data, lm] ++ List.wrap(args),
      opts
    )
  end

  @doc """
  Optimize the student program.



  ## Parameters

  - `student` - The student program to optimize.
  - `trainset` - The training set to use for optimization.
  - `teacher` - The teacher program to use for optimization.
  - `valset` - The validation set to use for optimization.

  ## Returns

  - `term()`
  """
  @spec compile(SnakeBridge.Ref.t(), term(), list(term()), list(term()), keyword()) ::
          {:ok, term()} | {:error, Snakepit.Error.t()}
  def compile(ref, student, trainset, args, opts \\ []) do
    {args, opts} = SnakeBridge.Runtime.normalize_args_opts(args, opts)
    SnakeBridge.Runtime.call_method(ref, :compile, [student, trainset] ++ List.wrap(args), opts)
  end

  @doc """
  Python method `BootstrapFinetune.convert_to_lm_dict`.

  ## Parameters

  - `arg` (term())

  ## Returns

  - `%{optional(term()) => term()}`
  """
  @spec convert_to_lm_dict(SnakeBridge.Ref.t(), term(), keyword()) ::
          {:ok, %{optional(term()) => term()}} | {:error, Snakepit.Error.t()}
  def convert_to_lm_dict(ref, arg, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :convert_to_lm_dict, [arg], opts)
  end

  @doc """
  Python method `BootstrapFinetune.finetune_lms`.

  ## Parameters

  - `finetune_dict` (term())

  ## Returns

  - `%{optional(term()) => term()}`
  """
  @spec finetune_lms(SnakeBridge.Ref.t(), term(), keyword()) ::
          {:ok, %{optional(term()) => term()}} | {:error, Snakepit.Error.t()}
  def finetune_lms(ref, finetune_dict, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :finetune_lms, [finetune_dict], opts)
  end

  @doc """
  Get the parameters of the teleprompter.

  ## Returns

  - `%{optional(String.t()) => term()}`
  """
  @spec get_params(SnakeBridge.Ref.t(), keyword()) ::
          {:ok, %{optional(String.t()) => term()}} | {:error, Snakepit.Error.t()}
  def get_params(ref, opts \\ []) do
    SnakeBridge.Runtime.call_method(ref, :get_params, [], opts)
  end
end
